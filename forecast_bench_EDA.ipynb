{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64fde7ac",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92acb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.information_retrieval import run_commit_retrieval, create_folders, download_csv_files, create_master_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b3b934",
   "metadata": {},
   "source": [
    "### Inital Params\n",
    "These are needed to scrape the github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "commits_url = \"https://api.github.com/repos/forecastingresearch/forecastbench-datasets/commits\"\n",
    "params = {'path': 'leaderboards/csv/leaderboard_overall.csv', 'per_page': 100}\n",
    "headers = {'Authorization': f'token {os.getenv(\"GITHUB_API_KEY\")}'}\n",
    "# Do make sure your API key has not expired when running this. \n",
    "# If it expired, you'll run into an infite for loop. Just make a new to fix it. Always make sure your API key expires as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555eeb95",
   "metadata": {},
   "source": [
    "### Information Retrieval Pipeline\n",
    "1. We create the folders needed to store the data\n",
    "2. We get the commit history from Forecast Bench and save it locally\n",
    "3. We retrieve all leaderboard csv files in accordance to the commit history\n",
    "4. We concatenate all leaderboard csv files into a single csv file to then do data analysis on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73458cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_commit_retrieval(commits_url, params, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a694ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "await download_csv_files(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_master_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0727022",
   "metadata": {},
   "source": [
    "### Setup Code to Group Each Model Together\n",
    "This code is some sample code I used to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbafd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the master csv file with all our information\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6615ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently choosing only questions that have actually been resolved, since a Brier Score on resolved questions made the most sense to me\n",
    "# It's unclear which column is actually best. The paper goes into more detail on what each category is\n",
    "filtered_df = df[['Organization', 'llm_model', 'method', 'date', 'Market Score (resolved)', 'Market Score (resolved) Dataset Size']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98964a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['date'] = pd.to_datetime(filtered_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284c94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.sort_values(by=['date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d3701",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a71d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954dc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all the dates that the benchmark has been active\n",
    "filtered_df['date'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236787d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a specific date\n",
    "sample_df = filtered_df[filtered_df['date']=='2024-12-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2705fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique list of models\n",
    "models = sample_df['llm_model'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54346e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each LLM provider on the previously specified date, print the info for that model\n",
    "# We are excluding forecastbench since they have some unique ways on how they do it\n",
    "# We really just care about how well the foundation models perform\n",
    "for model in models:\n",
    "    if (model == 'Forecastbench') or model == 'Llm Crowd Gpt-4O, Claude-3.5-Sonnet, Gemini-1.5-Pro':\n",
    "        continue\n",
    "    print(f\"\\nMODEL: {model.title()}\")\n",
    "    subset = sample_df[sample_df['llm_model'] == model]\n",
    "    subset_sorted = subset.sort_values(by=\"Market Score (resolved)\", ascending=True)\n",
    "    display(subset_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625790e",
   "metadata": {},
   "source": [
    "### Attempts At Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3eff93",
   "metadata": {},
   "source": [
    "##### This is for a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Load your actual data\n",
    "df = pd.read_csv('data/master.csv')\n",
    "\n",
    "# Filter for one model and multiple dates\n",
    "model_data = df[df['llm_model'] == 'Claude-3-5-Sonnet-20240620'].copy()\n",
    "\n",
    "print(f\"Found {len(model_data)} rows for Claude-3-5-Sonnet-20240620\")\n",
    "print(f\"Date range: {model_data['date'].min()} to {model_data['date'].max()}\")\n",
    "print(f\"Unique dates: {model_data['date'].nunique()}\")\n",
    "print(f\"Methods available: {sorted(model_data['method'].unique())}\")\n",
    "print()\n",
    "\n",
    "def analyze_freeze_vs_nonfreeze(df):\n",
    "    \"\"\"\n",
    "    Analyze best freeze methods vs best non-freeze methods\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Group by model and date\n",
    "    for (model, date), group in df.groupby(['llm_model', 'date']):\n",
    "        # Identify freeze vs non-freeze methods\n",
    "        freeze_methods = group[group['method'].str.contains('with freeze values', na=False)]\n",
    "        non_freeze_methods = group[~group['method'].str.contains('with freeze values', na=False)]\n",
    "        \n",
    "        if len(freeze_methods) > 0 and len(non_freeze_methods) > 0:\n",
    "            # Get best (lowest) scores\n",
    "            best_freeze_score = freeze_methods['Market Score (resolved)'].min()\n",
    "            best_freeze_method = freeze_methods.loc[freeze_methods['Market Score (resolved)'].idxmin(), 'method']\n",
    "            \n",
    "            best_non_freeze_score = non_freeze_methods['Market Score (resolved)'].min()\n",
    "            best_non_freeze_method = non_freeze_methods.loc[non_freeze_methods['Market Score (resolved)'].idxmin(), 'method']\n",
    "            \n",
    "            # Calculate difference (positive means freeze is better)\n",
    "            difference = best_non_freeze_score - best_freeze_score\n",
    "            \n",
    "            results.append({\n",
    "                'model': model,\n",
    "                'date': date,\n",
    "                'best_freeze_method': best_freeze_method,\n",
    "                'best_freeze_score': best_freeze_score,\n",
    "                'best_non_freeze_method': best_non_freeze_method,\n",
    "                'best_non_freeze_score': best_non_freeze_score,\n",
    "                'difference': difference\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n=== DEBUGGING INFO ===\")\n",
    "print(\"Sample of model_data:\")\n",
    "print(model_data[['method', 'date', 'Market Score (resolved)']].head(10))\n",
    "print()\n",
    "\n",
    "# Run the analysis on the filtered model data\n",
    "comparison_df = analyze_freeze_vs_nonfreeze(model_data)\n",
    "print(\"=== COMPARISON RESULTS ===\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Statistical analysis\n",
    "differences = comparison_df['difference'].values\n",
    "print(\"=== STATISTICAL ANALYSIS ===\")\n",
    "print(f\"Number of comparisons: {len(differences)}\")\n",
    "print(f\"Mean difference: {differences.mean():.4f}\")\n",
    "print(f\"Std deviation: {differences.std():.4f}\")\n",
    "print(f\"Positive differences (freeze better): {sum(differences > 0)}/{len(differences)}\")\n",
    "print()\n",
    "\n",
    "# One-tailed paired t-test\n",
    "# H0: no difference (mean difference = 0)\n",
    "# H1: freeze methods are better (mean difference > 0)\n",
    "if len(differences) > 1:\n",
    "    t_stat, p_value_two_tailed = stats.ttest_1samp(differences, 0)\n",
    "    p_value_one_tailed = p_value_two_tailed / 2 if t_stat > 0 else 1 - (p_value_two_tailed / 2)\n",
    "    \n",
    "    print(\"=== T-TEST RESULTS ===\")\n",
    "    print(f\"t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"p-value (one-tailed): {p_value_one_tailed:.4f}\")\n",
    "    print(f\"Effect size (Cohen's d): {differences.mean() / differences.std():.4f}\")\n",
    "    \n",
    "    if p_value_one_tailed < 0.05:\n",
    "        print(\"✅ SIGNIFICANT: Freeze methods significantly outperform non-freeze methods\")\n",
    "    else:\n",
    "        print(\"❌ NOT SIGNIFICANT: No significant difference found\")\n",
    "else:\n",
    "    print(\"Need multiple dates for statistical test\")\n",
    "\n",
    "print(\"\\n=== FOR YOUR REAL ANALYSIS ===\")\n",
    "print(\"1. Load your full dataset with multiple dates\")\n",
    "print(\"2. Filter for one model at a time\") \n",
    "print(\"3. Run this same analysis\")\n",
    "print(\"4. With 30+ dates, you'll have good statistical power\")\n",
    "\n",
    "# Example of how to load and filter your real data:\n",
    "print(\"\\n=== EXAMPLE CODE FOR YOUR FULL DATASET ===\")\n",
    "print(\"\"\"\n",
    "# Load your data\n",
    "df = pd.read_csv('your_forecast_data.csv')\n",
    "\n",
    "# Filter for one model and multiple dates\n",
    "model_data = df[df['llm_model'] == 'Claude-3-5-Sonnet-20240620'].copy()\n",
    "\n",
    "# Run the analysis\n",
    "results = analyze_freeze_vs_nonfreeze(model_data)\n",
    "print(results)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fc6076",
   "metadata": {},
   "source": [
    "### This is for analyzing all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your actual data\n",
    "df = pd.read_csv('data/master.csv')\n",
    "\n",
    "def analyze_all_freeze_vs_nonfreeze(df):\n",
    "    \"\"\"\n",
    "    Compare ALL freeze methods vs ALL non-freeze methods (not just best ones)\n",
    "    This gives us much more statistical power\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Group by model and date\n",
    "    for (model, date), group in df.groupby(['llm_model', 'date']):\n",
    "        # Identify freeze vs non-freeze methods\n",
    "        freeze_methods = group[group['method'].str.contains('with freeze values', na=False)]\n",
    "        non_freeze_methods = group[~group['method'].str.contains('with freeze values', na=False)]\n",
    "        \n",
    "        if len(freeze_methods) > 0 and len(non_freeze_methods) > 0:\n",
    "            # Get average scores for each category\n",
    "            avg_freeze_score = freeze_methods['Market Score (resolved)'].mean()\n",
    "            avg_non_freeze_score = non_freeze_methods['Market Score (resolved)'].mean()\n",
    "            \n",
    "            # Also get best scores\n",
    "            best_freeze_score = freeze_methods['Market Score (resolved)'].min()\n",
    "            best_non_freeze_score = non_freeze_methods['Market Score (resolved)'].min()\n",
    "            \n",
    "            # Calculate differences (positive means freeze is better)\n",
    "            avg_difference = avg_non_freeze_score - avg_freeze_score\n",
    "            best_difference = best_non_freeze_score - best_freeze_score\n",
    "            \n",
    "            results.append({\n",
    "                'model': model,\n",
    "                'date': date,\n",
    "                'avg_freeze_score': avg_freeze_score,\n",
    "                'avg_non_freeze_score': avg_non_freeze_score,\n",
    "                'avg_difference': avg_difference,\n",
    "                'best_freeze_score': best_freeze_score,\n",
    "                'best_non_freeze_score': best_non_freeze_score,\n",
    "                'best_difference': best_difference,\n",
    "                'n_freeze_methods': len(freeze_methods),\n",
    "                'n_non_freeze_methods': len(non_freeze_methods)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def run_statistical_tests(comparison_df, model_name):\n",
    "    \"\"\"\n",
    "    Run comprehensive statistical tests on the comparison data\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"STATISTICAL ANALYSIS FOR {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Test both average and best comparisons\n",
    "    for comparison_type in ['avg', 'best']:\n",
    "        diff_col = f'{comparison_type}_difference'\n",
    "        differences = comparison_df[diff_col].values\n",
    "        \n",
    "        print(f\"\\n--- {comparison_type.upper()} METHOD COMPARISON ---\")\n",
    "        print(f\"Number of comparisons: {len(differences)}\")\n",
    "        print(f\"Mean difference: {differences.mean():.4f}\")\n",
    "        print(f\"Std deviation: {differences.std():.4f}\")\n",
    "        print(f\"Positive differences (freeze better): {sum(differences > 0)}/{len(differences)}\")\n",
    "        print(f\"Success rate: {sum(differences > 0)/len(differences)*100:.1f}%\")\n",
    "        \n",
    "        # One-tailed paired t-test\n",
    "        if len(differences) > 1:\n",
    "            t_stat, p_value_two_tailed = stats.ttest_1samp(differences, 0)\n",
    "            p_value_one_tailed = p_value_two_tailed / 2 if t_stat > 0 else 1 - (p_value_two_tailed / 2)\n",
    "            \n",
    "            print(f\"t-statistic: {t_stat:.4f}\")\n",
    "            print(f\"p-value (one-tailed): {p_value_one_tailed:.6f}\")\n",
    "            \n",
    "            if differences.std() > 0:\n",
    "                cohens_d = differences.mean() / differences.std()\n",
    "                print(f\"Effect size (Cohen's d): {cohens_d:.4f}\")\n",
    "            else:\n",
    "                print(\"Effect size: Cannot calculate (no variation)\")\n",
    "            \n",
    "            if p_value_one_tailed < 0.05:\n",
    "                print(\"✅ SIGNIFICANT: Freeze methods significantly outperform non-freeze methods\")\n",
    "            else:\n",
    "                print(\"❌ NOT SIGNIFICANT: No significant difference found\")\n",
    "        else:\n",
    "            print(\"Need multiple dates for statistical test\")\n",
    "\n",
    "def analyze_single_model(df, model_name):\n",
    "    \"\"\"\n",
    "    Run complete analysis for a single model\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ANALYZING MODEL: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Filter for this model\n",
    "    model_data = df[df['llm_model'] == model_name].copy()\n",
    "    \n",
    "    if len(model_data) == 0:\n",
    "        print(f\"No data found for {model_name}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(model_data)} rows\")\n",
    "    print(f\"Date range: {model_data['date'].min()} to {model_data['date'].max()}\")\n",
    "    print(f\"Unique dates: {model_data['date'].nunique()}\")\n",
    "    print(f\"Methods available: {sorted(model_data['method'].unique())}\")\n",
    "    \n",
    "    # Check if we have both freeze and non-freeze methods\n",
    "    freeze_methods = model_data[model_data['method'].str.contains('with freeze values', na=False)]['method'].unique()\n",
    "    non_freeze_methods = model_data[~model_data['method'].str.contains('with freeze values', na=False)]['method'].unique()\n",
    "    \n",
    "    print(f\"\\nFreeze methods ({len(freeze_methods)}): {list(freeze_methods)}\")\n",
    "    print(f\"Non-freeze methods ({len(non_freeze_methods)}): {list(non_freeze_methods)}\")\n",
    "    \n",
    "    if len(freeze_methods) == 0 or len(non_freeze_methods) == 0:\n",
    "        print(\"⚠️ Cannot compare - missing freeze or non-freeze methods\")\n",
    "        return None\n",
    "    \n",
    "    # Run the analysis\n",
    "    comparison_df = analyze_all_freeze_vs_nonfreeze(model_data)\n",
    "    \n",
    "    if len(comparison_df) == 0:\n",
    "        print(\"No valid comparisons found\")\n",
    "        return None\n",
    "    \n",
    "    # Run statistical tests\n",
    "    run_statistical_tests(comparison_df, model_name)\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "def analyze_all_models(df):\n",
    "    \"\"\"\n",
    "    Run analysis across all models\n",
    "    \"\"\"\n",
    "    models = df['llm_model'].unique()\n",
    "    results_summary = []\n",
    "    \n",
    "    for model in models:\n",
    "        if model in ['Forecastbench', 'Llm Crowd Gpt-4O, Claude-3.5-Sonnet, Gemini-1.5-Pro']:\n",
    "            continue\n",
    "            \n",
    "        comparison_df = analyze_single_model(df, model)\n",
    "        \n",
    "        if comparison_df is not None and len(comparison_df) > 0:\n",
    "            # Summary statistics for this model\n",
    "            avg_diffs = comparison_df['avg_difference'].values\n",
    "            best_diffs = comparison_df['best_difference'].values\n",
    "            \n",
    "            if len(avg_diffs) > 1:\n",
    "                # Average method comparison\n",
    "                t_stat_avg, p_val_avg = stats.ttest_1samp(avg_diffs, 0)\n",
    "                p_val_avg_one_tailed = p_val_avg / 2 if t_stat_avg > 0 else 1 - (p_val_avg / 2)\n",
    "                \n",
    "                # Best method comparison  \n",
    "                t_stat_best, p_val_best = stats.ttest_1samp(best_diffs, 0)\n",
    "                p_val_best_one_tailed = p_val_best / 2 if t_stat_best > 0 else 1 - (p_val_best / 2)\n",
    "                \n",
    "                results_summary.append({\n",
    "                    'model': model,\n",
    "                    'n_comparisons': len(avg_diffs),\n",
    "                    'avg_success_rate': sum(avg_diffs > 0) / len(avg_diffs),\n",
    "                    'avg_mean_diff': avg_diffs.mean(),\n",
    "                    'avg_p_value': p_val_avg_one_tailed,\n",
    "                    'best_success_rate': sum(best_diffs > 0) / len(best_diffs),\n",
    "                    'best_mean_diff': best_diffs.mean(),\n",
    "                    'best_p_value': p_val_best_one_tailed\n",
    "                })\n",
    "    \n",
    "    # Print summary table\n",
    "    if results_summary:\n",
    "        summary_df = pd.DataFrame(results_summary)\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(\"SUMMARY ACROSS ALL MODELS\")\n",
    "        print(f\"{'='*100}\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "        \n",
    "        # Count significant results\n",
    "        sig_avg = sum(summary_df['avg_p_value'] < 0.05)\n",
    "        sig_best = sum(summary_df['best_p_value'] < 0.05)\n",
    "        total = len(summary_df)\n",
    "        \n",
    "        print(f\"\\nModels with significant freeze advantage:\")\n",
    "        print(f\"Average method comparison: {sig_avg}/{total} ({sig_avg/total*100:.1f}%)\")\n",
    "        print(f\"Best method comparison: {sig_best}/{total} ({sig_best/total*100:.1f}%)\")\n",
    "\n",
    "# Run the comprehensive analysis\n",
    "print(\"Starting comprehensive freeze vs non-freeze analysis...\")\n",
    "analyze_all_models(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
